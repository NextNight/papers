#### PEFT系列内容相关论文

- *(2019-02-02)* **Adapte**r:[Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/1902.00751) ✅
- *(2020-05-01)* **AdapterFusion** : [Non-Destructive Task Composition for Transfer Learning](http://arxiv.org/abs/2005.00247)✅

- *(2021-01-01)* **Prefix-Tuning**: [Optimizing Continuous Prompts for Generation](http://arxiv.org/abs/2101.00190)✅
- *(2021-01-01)* **WARP** : [Word-level Adversarial ReProgramming](http://arxiv.org/abs/2101.00121)✅
- *(2021-03-18)* **P-tuning** :[GPT Understands, Too✅](http://arxiv.org/abs/2103.10385)
- *(2021-04-18)* **Prompt-Tuning**:[The Power of Scale for Parameter-Efficient Prompt Tuning](http://arxiv.org/abs/2104.08691)✅
- *(2021-06-17)* **LoRA**: [Low-Rank Adaptation of Large Language Models](http://arxiv.org/abs/2106.09685)✅
- *(2021-06-18)* **BitFit** : [Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](http://arxiv.org/abs/2106.10199)✅
- *(2021-10-14)* **P-Tuning v2** : [Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](http://arxiv.org/abs/2110.07602)
- *(2021-10-08)* **MAM**：[Towards a Unified View of Parameter-Efficient Transfer Learning](http://arxiv.org/abs/2110.04366)✅
- *(2022-05-11)* **IA^3** :[Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](http://arxiv.org/abs/2205.05638)
- ...

#### PEFT系列相关框架

- *(2020-07-15)* **AdapterHub**: [A Framework for Adapting Transformers](http://arxiv.org/abs/2007.07779)
  - 提出了用于PEFT的框架AdapterHub：https://adapterhub.ml/
- *(2022-03-14)* **Delta Tuning**: [A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models](http://arxiv.org/abs/2203.06904)
  - 介绍各种peft-tuning的性能，同时详解了内部原理.
  - 并构建了一个用于PEFT的框架OpenDelta：https://github.com/thunlp/OpenDelta

- *(2022-11-25)* **PEFT**
  - hagging face 构建的一个用于PEFT的框架PEFT:https://github.com/huggingface/peft


